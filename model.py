import os
import pandas as pd
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA

# Load .env only once
load_dotenv(".env")
hugging_face_api_key = os.getenv("HUGGINGFACE_API_KEY")
groq_api_key = os.getenv("GROQ_API_KEY")

# Load Excel and convert to LangChain Documents
excel_data = pd.read_csv("youtube_csv.csv")
excel_documents = [
    Document(page_content=row["input"], metadata={"youtube_url": row["output"]})
    for _, row in excel_data.iterrows()
]

# Load PDF and split into documents
pdf_docs = PyMuPDFLoader("icar_data_updated.pdf").load()
splitter = RecursiveCharacterTextSplitter(
    chunk_size=750, chunk_overlap=150, separators=["\n\n", "\n", ".", " "]
)
split_documents = splitter.split_documents(pdf_docs)

# Create embeddings (loaded once)
embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Vector DBs (loaded once)
pdf_vector_db = FAISS.from_documents(split_documents, embedding)
pdf_retriever = pdf_vector_db.as_retriever()

youtube_vector_db = FAISS.from_documents(excel_documents, embedding)
youtube_retriever = youtube_vector_db.as_retriever()

# Load LLM once
llm = ChatGroq(model="Llama-3.3-70b-Versatile", groq_api_key=groq_api_key)

# Prompt template
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    ‡§§‡•Å‡§Æ ‡§è‡§ï ‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§ï‡•É‡§∑‡§ø ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•ã, ‡§ú‡§ø‡§∏‡•á ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•ã‡§Ø‡§æ‡§¨‡•Ä‡§® ‡§ñ‡•á‡§§‡•Ä ‡§∏‡•á ‡§ú‡•Å‡§°‡§º‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§¶‡•á‡§®‡§æ ‡§π‡•à‡•§ 
    ‡§§‡•Å‡§Æ‡•ç‡§π‡•á‡§Ç ‡§®‡•Ä‡§ö‡•á ‡§¶‡§ø‡§è ‡§ó‡§è "‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠" (context) ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§π‡•Ä ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§®‡§æ ‡§π‡•à ‚Äî ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§∏‡•á ‡§¨‡§æ‡§π‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è‡•§

    ‡§â‡§§‡•ç‡§§‡§∞ ‡§á‡§∏ ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è:
    1. ‡§è‡§ï ‡§õ‡•ã‡§ü‡§æ ‡§î‡§∞ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï (Title) ‡§≤‡§ø‡§ñ‡•ã
    2. ‡§¨‡•Å‡§≤‡•á‡§ü ‡§™‡•â‡§á‡§Ç‡§ü‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§∏‡§∞‡§≤ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§¶‡•ã
    3. ‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡•Ç‡§∞‡•Ä ‡§§‡§∞‡§π ‡§∏‡•á ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è ‡§§‡§æ‡§ï‡§ø ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§® ‡§Ü‡§∏‡§æ‡§®‡•Ä ‡§∏‡•á ‡§∏‡§Æ‡§ù ‡§∏‡§ï‡•á

    ‡§Ö‡§ó‡§∞ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§§‡•ã ‡§≤‡§ø‡§ñ‡•ã:  
    "‡§Ø‡§π ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§"

    ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:
    1. ‡§â‡§§‡•ç‡§§‡§∞ ‡§π‡§Æ‡•á‡§∂‡§æ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¶‡•ã
    2. ‡§â‡§§‡•ç‡§§‡§∞ ‡§ï‡•ã ‡§è‡§ï ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•ã
    3. ‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï‡§ø‡§§ ‡§¨‡§ø‡§Ç‡§¶‡•Å‡§ì‡§Ç (1, 2, 3...) ‡§Æ‡•á‡§Ç ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ã
    4. ‡§™‡•ç‡§∞‡§§‡•Ä‡§ï ‡§ö‡§ø‡§π‡•ç‡§®‡•ã‡§Ç ‡§ï‡§æ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§Æ‡§§ ‡§ï‡§∞‡•ã
    5. ‡§ú‡§µ‡§æ‡§¨ ‡§ï‡•á‡§µ‡§≤ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§∏‡•á ‡§¶‡•ã
    6. ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á ‡§§‡•ã ‡§≤‡§ø‡§ñ‡•ã: "‡§Ø‡§π ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§"
    7. ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§á‡§Æ‡•ã‡§ú‡•Ä (üåæ, üêõ, üíß, üß™ ‡§Ü‡§¶‡§ø) ‡§≤‡§ó‡§æ‡§ì
    8. ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§π‡•ã ‡§§‡•ã ‡§§‡§æ‡§≤‡§ø‡§ï‡§æ ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ‡§ì

    ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠:
    {context}

    ‡§™‡•ç‡§∞‡§∂‡•ç‡§®:
    {question}

    ‡§â‡§§‡•ç‡§§‡§∞ (‡§∏‡§ø‡§∞‡•ç‡§´ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç, ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï ‡§î‡§∞ ‡§¨‡§ø‡§Ç‡§¶‡•Å‡§ì‡§Ç ‡§∏‡§π‡§ø‡§§):
    """
)

# QA Chain (created once)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=pdf_retriever,
    chain_type_kwargs={"prompt": prompt_template}
)

# Only this function runs every time (fast)
def final_model(query):
    # PDF Answer
    response_from_pdf_file = qa_chain.run(query)

    # Youtube Links from Excel
    docs = youtube_retriever.get_relevant_documents(query)
    if not docs:
        formatted_links = "‡§ï‡•ã‡§à ‡§≤‡§ø‡§Ç‡§ï ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§"
    else:
        youtube_links = docs[0].metadata["youtube_url"]
        split_links = [link.strip() for link in youtube_links.split(",")]
        formatted_links = "\n".join(split_links)

    return f"Question: {query}\n\nAnswer:\n{response_from_pdf_file}\n\nSuggested Youtube Links:\n{formatted_links}"
